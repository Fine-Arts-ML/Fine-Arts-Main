{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be81eb4",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "by far the slowest with Ëœ5min/prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c53af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/Fine-arts-ML/Fine-Arts-Main/.venv3_12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 00:45:46 [__init__.py:241] Automatically detected platform cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from huggingface_hub import login\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1322ac11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_cache/aquarell_miro_shapes4_kopie.jpg',\n",
       " 'image_cache/aqua_shapes4_kopie.jpg',\n",
       " 'image_cache/aquarell_miro_shapes3e_kopie.jpg',\n",
       " 'image_cache/aqua_shapes2_kopie.jpg',\n",
       " 'image_cache/aquarell_miro_shapes_kopie.jpg',\n",
       " 'image_cache/aqua_shapes3_kopie.jpg',\n",
       " 'image_cache/ausschnitte_aus_der_mitte_des_jahrhunderts.jpg',\n",
       " 'image_cache/ausschnitte_bauhaus_2_kopie.jpg',\n",
       " 'image_cache/ausschnitte_aus_der_mitte_des_jahrhunderts2.jpg',\n",
       " 'image_cache/aqua_shapes1_kopie.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection details\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")\n",
    "working_dir = os.getcwd()\n",
    "full_path = os.path.realpath(cache_dir)\n",
    "image_cache_list = os.listdir(cache_dir)\n",
    "image_list = []\n",
    "for pic in image_cache_list:\n",
    "    pic_path = os.path.join(cache_dir, pic)\n",
    "    image_list.append(pic_path)\n",
    "\n",
    "image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0318cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_http_server(port=8000, directory=\".\"):\n",
    "    handler = http.server.SimpleHTTPRequestHandler\n",
    "    os.chdir(directory)\n",
    "\n",
    "    httpd = socketserver.TCPServer((\"\", port), handler)\n",
    "\n",
    "    thread = threading.Thread(target=httpd.serve_forever, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    print(f\"Serving images at http://localhost:{port}\")\n",
    "    return httpd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874c922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_url(local_image_path, port=8000):\n",
    "    # Get the file name\n",
    "    image_name = os.path.basename(local_image_path)\n",
    "\n",
    "    image_url = f\"http://localhost:{port}/{image_name}\"\n",
    "    return image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98761c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 00:46:46 [utils.py:326] non-default args: {'model': 'mistralai/Pixtral-12B-2409', 'tokenizer_mode': 'mistral', 'load_format': 'mistral', 'dtype': 'float16', 'max_model_len': 4096, 'disable_log_stats': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 00:46:49 [config.py:726] Resolved architecture: PixtralForConditionalGeneration\n",
      "ERROR 08-07 00:46:49 [config.py:123] Error retrieving safetensors: 'mistralai/Pixtral-12B-2409' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files., retrying 1 of 2\n",
      "ERROR 08-07 00:46:52 [config.py:121] Error retrieving safetensors: 'mistralai/Pixtral-12B-2409' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.\n",
      "INFO 08-07 00:46:52 [config.py:3643] Downcasting torch.float32 to torch.float16.\n",
      "INFO 08-07 00:46:52 [config.py:1765] Using max model len 4096\n",
      "WARNING 08-07 00:46:52 [cpu.py:113] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 08-07 00:46:52 [arg_utils.py:1069] Chunked prefill is not supported for ARM and POWER CPUs; disabling it for V1 backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/Fine-arts-ML/Fine-Arts-Main/.venv3_12/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 00:46:54 [__init__.py:241] Automatically detected platform cpu.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:54 [core.py:619] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:54 [core.py:71] Initializing a V1 LLM engine (v0.10.1.dev405+g31f09c615) with config: model='mistralai/Pixtral-12B-2409', speculative_config=None, tokenizer='mistralai/Pixtral-12B-2409', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=mistral, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Pixtral-12B-2409, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":2,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false,\"dce\":true,\"size_asserts\":false,\"nan_asserts\":false,\"epilogue_fusion\":true},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:55 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:55 [cpu_worker.py:62] Warning: NUMA is not enabled in this build. `init_cpu_threads_env` has no effect to setup thread affinity.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:55 [parallel_state.py:1124] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m WARNING 08-07 00:46:55 [cpu.py:304] Pin memory is not supported on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m /Users/tom/Fine-arts-ML/Fine-Arts-Main/.venv3_12/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m /Users/tom/Fine-arts-ML/Fine-Arts-Main/.venv3_12/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (SpecialTokenPolicy.IGNORE) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m WARNING 08-07 00:46:56 [registry.py:183] PixtralProcessorAdapter did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:56 [cpu_model_runner.py:87] Starting to load model mistralai/Pixtral-12B-2409...\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m WARNING 08-07 00:46:56 [cpu.py:113] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:56 [cpu.py:100] Using Torch SDPA backend.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:57 [weight_utils.py:296] Using model weights format ['consolidated*.safetensors', '*.pt']\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:46:57 [weight_utils.py:349] No consolidated.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.29s/it]\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:02 [default_loader.py:262] Loading weights took 5.40 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:02 [kv_cache_utils.py:829] GPU KV cache size: 26,208 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:02 [kv_cache_utils.py:833] Maximum concurrency for 4,096 tokens per request: 6.40x\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:02 [cpu.py:100] Using Torch SDPA backend.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:03 [cpu_model_runner.py:99] Warming up model for the compilation...\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:15 [cpu_model_runner.py:103] Warming up done.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m INFO 08-07 00:47:15 [core.py:199] init engine (profile, create kv cache, warmup model) took 12.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m /Users/tom/Fine-arts-ML/Fine-Arts-Main/.venv3_12/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/tekken.py:337: FutureWarning: The attributed `special_token_policy` is deprecated and will be removed in 1.10.0. Please pass a special token policy explicitly to the relevant methods.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m /Users/tom/Fine-arts-ML/Fine-Arts-Main/.venv3_12/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/tekken.py:461: FutureWarning: Using the tokenizer's special token policy (SpecialTokenPolicy.IGNORE) is deprecated. It will be removed in 1.10.0. Please pass a special token policy explicitly. Future default will be SpecialTokenPolicy.IGNORE.\n",
      "\u001b[1;36m(EngineCore_0 pid=3772)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 00:47:16 [llm.py:290] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "def process_image_with_llm(local_image_path):\n",
    "    login(token=\"ENTER YOU HF TOCKEN\")\n",
    "\n",
    "# Define the model and sampling parameters\n",
    "model_name = \"mistralai/Pixtral-12B-2409\"\n",
    "sampling_params = SamplingParams(max_tokens=8192)\n",
    "\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tokenizer_mode=\"mistral\",\n",
    "    load_format=\"mistral\",\n",
    "    dtype='float16',  # Use 16-bit precision\n",
    "    max_model_len=4096,  # Adjust max model length if necessary\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb55538",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Start the HTTP server to serve the image\u001b[39;00m\n\u001b[32m      7\u001b[39m port = \u001b[32m8000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m httpd = \u001b[43mstart_http_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_cache/aquarell_miro_shapes4_kopie.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Get the URL of the image\u001b[39;00m\n\u001b[32m     13\u001b[39m image_url = get_image_url(\u001b[33m'\u001b[39m\u001b[33mimage_cache/aquarell_miro_shapes4_kopie.jpg\u001b[39m\u001b[33m'\u001b[39m, port=port)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mstart_http_server\u001b[39m\u001b[34m(port, directory)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart_http_server\u001b[39m(port=\u001b[32m8000\u001b[39m, directory=\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      2\u001b[39m     handler = http.server.SimpleHTTPRequestHandler\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     httpd = socketserver.TCPServer((\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, port), handler)\n\u001b[32m      7\u001b[39m     thread = threading.Thread(target=httpd.serve_forever, daemon=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'image_cache'"
     ]
    }
   ],
   "source": [
    "# Define the prompt for extracting details\n",
    "\n",
    "\n",
    "prompt = \"\"\"Descirbe the Image. \"\"\"\n",
    "\n",
    "# Start the HTTP server to serve the image\n",
    "port = 8000\n",
    "httpd = start_http_server(port=port, directory=os.path.dirname('image_cache/aquarell_miro_shapes4_kopie.jpg'))\n",
    "\n",
    "\n",
    "\n",
    "# Get the URL of the image\n",
    "image_url = get_image_url('image_cache/aquarell_miro_shapes4_kopie.jpg', port=port)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the input messages for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Get the model's response\n",
    "outputs = llm.chat(messages=messages, sampling_params=sampling_params)\n",
    "\n",
    "# Output the results\n",
    "print(outputs[0].outputs[0].text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232f1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gracefully stop the server after completion\n",
    "httpd.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5min / prompt, leider langsamer als das community model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
