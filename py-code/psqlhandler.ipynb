{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c373ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pg8000\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import uniqid\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca1ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_db_connection():\n",
    "    DB_HOST = os.getenv(\"DB_HOST\")\n",
    "    DB_NAME = os.getenv(\"DB_NAME\")\n",
    "    DB_USER = os.getenv(\"DB_USER\")\n",
    "    DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "    engine = create_engine('postgresql+pg8000://'+DB_USER+':'+DB_PASSWORD+'@'+DB_HOST+':5432/'+DB_NAME)\n",
    "    return engine\n",
    "\n",
    "def get_file_locs(engine, SELECT, FROM, LIKE, ORDER, LIMIT):\n",
    "\n",
    "    Querry_string = f\"SELECT {SELECT} FROM {FROM} WHERE path like {LIKE} ORDER BY {ORDER} DESC LIMIT {LIMIT}\"\n",
    "    \n",
    "    try:\n",
    "        df_file_loc = pd.read_sql_query(sqlalchemy.text(Querry_string), engine)\n",
    "        print(\"got file paths\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    Querry_string_user = f\"SELECT * FROM public.oc_storages ORDER BY numeric_id ASC LIMIT {LIMIT}\"\n",
    "    try:\n",
    "        df_user_loc = pd.read_sql_query(sqlalchemy.text(Querry_string_user), engine)\n",
    "        print(\"done\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    \n",
    "    df_user_loc['id_clean'] = df_user_loc['id'].str.replace('(home::|local::)', '', regex=True)\n",
    "    df_file_loc = pd.merge(df_file_loc, df_user_loc[['numeric_id', 'id_clean']], \n",
    "          left_on='storage', \n",
    "          right_on='numeric_id', \n",
    "          how='left')\n",
    "         \n",
    "    df_file_loc['full_path'] = df_file_loc['id_clean'].astype(str) + '/' + df_file_loc['path'].astype(str)\n",
    "\n",
    "    return df_file_loc\n",
    "\n",
    "\n",
    "def get_files(df_file_loc):\n",
    "    # Base remote path\n",
    "    remote_base = os.getenv(\"REMOTE_BASE_PATH\")\n",
    "\n",
    "    # password and user\n",
    "    password = os.getenv(\"SSHPASS\")\n",
    "    ssh_user = os.getenv(\"SSHUSER\")\n",
    "    ssh_ip = os.getenv(\"SSHIP\")\n",
    "\n",
    "    # Create a cache directory if it doesn't exist\n",
    "    cache_dir = os.getenv(\"CACHE_DIR\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    print(f\"found {len(df_file_loc)} files to copy\")\n",
    "    # SSH command to list files\n",
    "    for path in tqdm(df_file_loc['full_path']):\n",
    "        # Clean the path (remove 'files/' prefix)\n",
    "        full_path = f\"{remote_base}{path}\"\n",
    "        # Escape spaces in the path for shell commands\n",
    "        escaped_path = full_path.replace(' ', '\\\\ ')\n",
    "        \n",
    "        # Execute SSH command with sudo and provide password through stdin\n",
    "        cmd = f'''sshpass -p {password} ssh -o StrictHostKeyChecking=no {ssh_user}@{ssh_ip} \"echo {password} | sudo -S ls -l '{full_path}'\"'''\n",
    "        try:\n",
    "            result = subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "\n",
    "            # Extract the filename from the path\n",
    "            filename = os.path.basename(path)\n",
    "            local_path = os.path.join(cache_dir, filename)\n",
    "\n",
    "            # Use sudo with SCP to copy the file\n",
    "            scp_cmd = f'''sshpass -p {password} ssh -o StrictHostKeyChecking=no {ssh_user}@{ssh_ip} \"echo {password} | sudo -S cat '{full_path}'\" > \"{local_path}\"'''\n",
    "            scp_result = subprocess.run(scp_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "\n",
    "            if os.path.getsize(local_path) < 1:\n",
    "                print(f\"Error copying file {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing {full_path}: {str(e)}\")\n",
    "        df_file_loc['local_path'] =  local_path\n",
    "    return  df_file_loc\n",
    "\n",
    "def get_allready_tag_fileid (engine):\n",
    "    QUERRY = \"SELECT objectid, systemtagid FROM public.oc_systemtag_object_mapping\"\n",
    "    try:\n",
    "        df_fileid = pd.read_sql_query(sqlalchemy.text(QUERRY), engine)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    return df_fileid\n",
    "\n",
    "def write_tags_to_db(tags, engine):\n",
    "\n",
    "    #get the existing tags\n",
    "    QUERRY = 'SELECT * from public.oc_systemtag'\n",
    "    try:\n",
    "        existing_tags = pd.read_sql_query(sqlalchemy.text(QUERRY), engine)\n",
    "        print(\"got existing tags\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    existing_tags['name'] = existing_tags['name'].str.lower()\n",
    "\n",
    "    # Create a new DataFrame for new tags\n",
    "    tags['name'] = tags['name'].str.lower()\n",
    "    tags = tags.drop_duplicates(subset=['name'])\n",
    "   \n",
    "    # Create a new DataFrame for new tags\n",
    "    new_tags = pd.DataFrame(columns=['id', 'name', 'visibility', 'editable', 'etag', \"color\"])\n",
    "    new_tags['name'] = tags['name']\n",
    "        \n",
    "    # Filter out tags that already exist in the database\n",
    "    new_tags = new_tags[~new_tags['name'].isin(existing_tags['name'])]\n",
    "    if new_tags.empty:\n",
    "        print(\"No new tags to insert into the database.\")\n",
    "        return    \n",
    "    print(f\"Inserting {len(new_tags)} new tags into the database.\")\n",
    "\n",
    "    # Ensure the 'id' column is of type int and assign new IDs\n",
    "    if existing_tags.empty:\n",
    "        new_tags['id'] = range(1, len(new_tags) + 1)\n",
    "    else:\n",
    "        new_tags['id'] = range(existing_tags['id'].max() + 1, existing_tags['id'].max() + len(new_tags) + 1)\n",
    "        \n",
    "    new_tags.apply(lambda x: pd.Series([1,1,\"\",\"\"], index =['visibility', 'editable', 'etag', 'color']), axis=1)\n",
    "    new_tags.apply(lambda x: x.astype(int) if x in new_tags['id', 'visibility', 'editable'] else x.astype(str), axis=1)\n",
    "\n",
    "    # Insert new tags into the database\n",
    "    new_tags.to_sql('oc_systemtag', engine, if_exists='append', index=False)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14390022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got existing tags\n"
     ]
    }
   ],
   "source": [
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "engine = create_engine('postgresql+pg8000://'+DB_USER+':'+DB_PASSWORD+'@'+DB_HOST+':5432/'+DB_NAME)\n",
    "\n",
    "QUERRY = \"SELECT objectid, systemtagid FROM public.oc_systemtag_object_mapping\"\n",
    "try:\n",
    "    df_fileid = pd.read_sql_query(sqlalchemy.text(QUERRY), engine)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "\n",
    "#get the existing tags\n",
    "QUERRY = 'SELECT * from public.oc_systemtag'\n",
    "try:\n",
    "    existing_tags = pd.read_sql_query(sqlalchemy.text(QUERRY), engine)\n",
    "    print(\"got existing tags\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "existing_tags['name'] = existing_tags['name'].str.lower()\n",
    "\n",
    "df_file_loc = pd.read_csv('image_cache/db.csv')\n",
    "uniq_tags = df_file_loc['tags']\n",
    "# Split the tags into separate rows\n",
    "tags_expanded = uniq_tags.str.split(';').explode().str.strip().replace('\"', '', regex=True)\n",
    "tags_expanded = tags_expanded[tags_expanded != '']\n",
    "tags_expanded = tags_expanded.drop_duplicates()\n",
    "tags_expanded = tags_expanded.reset_index(drop=True)\n",
    "uniq_tags = tags_expanded.to_frame(name='name')\n",
    "uniq_tags.to_csv(os.path.join('image_cache/tags.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "937b1ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 101 new tags into the database.\n"
     ]
    }
   ],
   "source": [
    "tags = uniq_tags\n",
    "\n",
    "# Drop duplicate tag names\n",
    "tags = tags.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Create a new DataFrame for new tags\n",
    "new_tags = pd.DataFrame(columns=['id', 'name', 'visibility', 'editable', 'etag', \"color\"])\n",
    "new_tags['name'] = tags['name']\n",
    "\n",
    "# Filter out tags that already exist in the database\n",
    "new_tags = new_tags[~new_tags['name'].isin(existing_tags['name'])]\n",
    "if new_tags.empty:\n",
    "    print(\"No new tags to insert into the database.\")\n",
    "else:\n",
    "    print(f\"Inserting {len(new_tags)} new tags into the database.\")\n",
    "\n",
    "    # Ensure the 'id' column is of type int and assign new IDs\n",
    "    if existing_tags.empty:\n",
    "        new_tags['id'] = range(1, len(new_tags) + 1)\n",
    "    else:\n",
    "        new_tags['id'] = range(existing_tags['id'].max() + 1, existing_tags['id'].max() + len(new_tags) + 1)\n",
    "\n",
    "    # Set default values for new columns\n",
    "    new_tags['visibility'] = 1\n",
    "    new_tags['editable'] = 1\n",
    "    new_tags['etag'] = [uniqid.uniqid() for _ in range(len(new_tags))]\n",
    "    new_tags['color'] = None\n",
    "\n",
    "    # Ensure correct types\n",
    "    new_tags['id'] = new_tags['id'].astype(int)\n",
    "    new_tags['visibility'] = new_tags['visibility'].astype(int)\n",
    "    new_tags['editable'] = new_tags['editable'].astype(int)\n",
    "    new_tags.index = new_tags.id\n",
    "\n",
    "    # Insert new tags into the database\n",
    "    new_tags.to_sql('oc_systemtag', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622f6052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_cache/db.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_dir = os.getenv(\"CACHE_DIR\")\n",
    "str = cache_dir + \"/db.csv\"\n",
    "\n",
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec61dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
